{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Rate Estimation using PPG (Convolutional Neural Networks Classification)\n",
    "\n",
    "The data has to be downloaded from [here](https://archive.ics.uci.edu/ml/datasets/PPG-DaLiA), unzipped into the current folder. Different folders for each individual has to be in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [02:02<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "lengths=[]\n",
    "activities = []\n",
    "ppg = []\n",
    "lines = []\n",
    "X = []\n",
    "for i in tqdm(range(1,16)):\n",
    "   st ='S'+str(i)+'/'+'S'+str(i)+'.pkl'\n",
    "   with open(st, 'rb') as f:\n",
    "       temp = pickle.load(f, encoding='latin1')\n",
    "   ppg = temp['signal']['wrist']['BVP'].tolist()\n",
    "   qppg =[]\n",
    "   lengths.append(int(len(ppg)/128)-3)\n",
    "   ac = temp['activity']\n",
    "   tempac =[]\n",
    "   for l in range(int(floor(ac.shape[0]/8))-3):\n",
    "       activities.append(max(ac[l*8:(l+4)*8]))\n",
    "   #print(len(activities),len(temp['label']))\n",
    "   for p in range(int(len(ppg)/128)-3):\n",
    "       t = np.zeros(512)\n",
    "       k = 0\n",
    "       for j in range(p*128,(4+p)*128):\n",
    "           t[k] = ppg[j][0]\n",
    "           k = k+1\n",
    "       qppg.append(t)\n",
    "   X.extend(qppg)\n",
    "   lines.extend(temp['label'])\n",
    "   #activities.extend(temp['activity'].tolist())\n",
    "   #print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "for i in lines:\n",
    "    t = np.zeros(1)\n",
    "    t[0]=i\n",
    "    Y.append(t)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, ceil\n",
    "y_class = []\n",
    "sort_dict = {}\n",
    "for rate in Y:\n",
    "   fl = floor(rate/5)\n",
    "   ce = ceil(rate/5)\n",
    "   st = str(fl*5) + \"-\" + str(ce*5)\n",
    "   y_class.append(st)\n",
    "   sort_dict[st] = fl-8\n",
    " \n",
    "y_class = np.array(y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['100-105', '105-110', '110-115', '115-120', '120-125', '125-130',\n",
       "       '130-135', '135-140', '140-145', '145-150', '150-155', '155-160',\n",
       "       '160-165', '165-170', '170-175', '175-180', '180-185', '185-190',\n",
       "       '40-45', '45-50', '50-55', '55-60', '60-65', '65-70', '70-75',\n",
       "       '75-80', '80-85', '85-90', '90-95', '95-100'], dtype='<U7')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = []\n",
    "for rate in y_class:\n",
    "    temp = np.zeros(30)\n",
    "    temp[sort_dict[rate]] = 1\n",
    "    y_one_hot.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = np.array(y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X and y_one_hot has the input and output data for training the model. The total data accounted for 64697 samples and each input sample is of size 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64697, 512) (64697, 30)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input and output data is spiltted into train and test set using sklearn's train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y_one_hot, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used keras to train the CNN model. The model has been defined with various layers as shown below and activated with relu for the hidden layers and compiled with Adam optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv1D, Reshape, MaxPooling1D, Flatten\n",
    "\n",
    "max_words=512\n",
    "model = Sequential()\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Reshape((1,4,128)))\n",
    "model.add(Conv1D(256,3,activation='relu',input_shape=[4,128]))\n",
    "model.add(Reshape((4,128)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Reshape((1,4,64)))\n",
    "model.add(Conv1D(256,3,activation='relu',input_shape=[4,128]))\n",
    "model.add(Reshape((4,128)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Reshape(([30])))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='Adam',loss='mse')\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use callbacks with early stopping with a patience of 15 and Model checkpoint to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='loss', patience=15),\n",
    "             ModelCheckpoint('cnn_classifier_model.h5', save_best_only=True, \n",
    "                             save_weights_only=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to load the trained model and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 300\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,callbacks=callbacks,validation_data=(X_test,y_test), verbose=1)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to load the trained model and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "loaded=keras.models.load_model('cnn_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below code to get the accuracies for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 4ms/step - loss: 0.2941 - accuracy: 0.9196\n"
     ]
    }
   ],
   "source": [
    "score = loaded.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910/910 [==============================] - 4s 4ms/step - loss: 0.3299 - accuracy: 0.9083\n"
     ]
    }
   ],
   "source": [
    "score = loaded.evaluate(X_train, y_train, batch_size=batch_size, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}